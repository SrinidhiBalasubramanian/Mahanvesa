{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a97b1a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roopesh/cs787/.venv/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faiss not available. Hard-negative mining will be skipped unless you install faiss-cpu or faiss-gpu.\n",
      "Loading train pairs from: ./dataset_out_simple/train_pairs_clean.jsonl\n",
      "Total raw pairs: 29489\n",
      "Prepared 29489 InputExample pairs (anchor, positive)\n",
      "Train examples: 28977 Dev examples: 512\n",
      "Training 2 epochs, 453 steps/epoch, total steps 906, warmup 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently using DataParallel (DP) for multi-gpu training, while DistributedDataParallel (DDP) is recommended for faster training. See https://sbert.net/docs/sentence_transformer/training/distributed.html for more information.\n",
      "/home/roopesh/cs787/.venv/lib/python3.8/site-packages/sentence_transformers/trainer.py:205: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SentenceTransformerTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute_loss() got an unexpected keyword argument 'num_items_in_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 129\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m    128\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(out_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 129\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmpnet_stage1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    136\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStage 1 training complete. Model saved at:\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmpnet_stage1\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# %% Evaluation: simple Recall@k on dev set using model + FAISS\u001b[39;00m\n",
      "File \u001b[0;32m/home/roopesh/cs787/.venv/lib/python3.8/site-packages/sentence_transformers/fit_mixin.py:374\u001b[0m, in \u001b[0;36mFitMixin.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m     trainer\u001b[38;5;241m.\u001b[39madd_callback(SaveModelCallback(output_path, evaluator, save_best_model))\n\u001b[0;32m--> 374\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/roopesh/cs787/.venv/lib/python3.8/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/roopesh/cs787/.venv/lib/python3.8/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/home/roopesh/cs787/.venv/lib/python3.8/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "\u001b[0;31mTypeError\u001b[0m: compute_loss() got an unexpected keyword argument 'num_items_in_batch'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "Contrastive fine-tuning notebook for multi-qa-mpnet on Mahabharata dataset.\n",
    "Inputs required:\n",
    " - train_pairs.jsonl : lines of {\"anchor\",\"positive\",\"negatives\",\"meta\"}\n",
    " - passages.jsonl (optional) : passages with \"text\" fields (for indexing/eval)\n",
    "Outputs:\n",
    " - saved model under out_dir (default ./mpnet_ft)\n",
    " - passage embeddings: out_dir/passage_embeddings.npy\n",
    " - faiss index: out_dir/faiss_index.idx\n",
    "\"\"\"\n",
    "# %% imports\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# huggingface / sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, util, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# optional faiss (install faiss-cpu or faiss-gpu)\n",
    "try:\n",
    "    import faiss\n",
    "except Exception:\n",
    "    faiss = None\n",
    "    print(\"faiss not available. Hard-negative mining will be skipped unless you install faiss-cpu or faiss-gpu.\")\n",
    "\n",
    "# %% user params - adjust as needed\n",
    "train_pairs_path = \"./dataset_out_simple/train_pairs_clean.jsonl\"  # path to your train pairs\n",
    "passages_path = \"./dataset_out_simple/passages_clean.jsonl\"        # optional, used for indexing\n",
    "out_dir = \"./mpnet_ft\"\n",
    "base_model = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "\n",
    "# training hyperparams\n",
    "batch_size = 64           # try 64, increase if GPU allows\n",
    "num_epochs = 2               # 1-3 is usually fine for contrastive\n",
    "learning_rate = 2e-5\n",
    "max_pairs_to_use = None      # set int to subsample, or None to use all\n",
    "warmup_ratio = 0.1           # proportion of steps for warmup\n",
    "\n",
    "# hard-negative mining params\n",
    "do_hard_negative_mining = True if faiss is not None else False\n",
    "mine_top_k = 64\n",
    "hard_negatives_per_anchor = 2\n",
    "hard_retrain_epochs = 1\n",
    "\n",
    "# device/mixed precision handled by sentence-transformers automatically if installed with pytorch+cuda\n",
    "\n",
    "# %% helper functions\n",
    "def read_jsonl(path):\n",
    "    items = []\n",
    "    with open(path, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            items.append(json.loads(line))\n",
    "    return items\n",
    "\n",
    "def save_model_and_embeddings(model, passages_texts, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    model.save(out_dir)\n",
    "    print(\"Saved model to\", out_dir)\n",
    "    if passages_texts is not None and len(passages_texts) > 0:\n",
    "        emb = model.encode(passages_texts, batch_size=256, convert_to_numpy=True, show_progress_bar=True)\n",
    "        np.save(os.path.join(out_dir, \"passage_embeddings.npy\"), emb)\n",
    "        print(\"Saved passage embeddings (shape {})\".format(emb.shape))\n",
    "        if faiss is not None:\n",
    "            # normalize for inner product search\n",
    "            faiss.normalize_L2(emb)\n",
    "            d = emb.shape[1]\n",
    "            index = faiss.IndexFlatIP(d)\n",
    "            index.add(emb)\n",
    "            faiss.write_index(index, os.path.join(out_dir, \"faiss_index.idx\"))\n",
    "            print(\"Built & saved FAISS index at\", os.path.join(out_dir, \"faiss_index.idx\"))\n",
    "    else:\n",
    "        print(\"No passages provided; skipping embedding/index save.\")\n",
    "\n",
    "# %% Load training pairs\n",
    "print(\"Loading train pairs from:\", train_pairs_path)\n",
    "pairs = read_jsonl(train_pairs_path)\n",
    "print(\"Total raw pairs:\", len(pairs))\n",
    "if max_pairs_to_use is not None and max_pairs_to_use < len(pairs):\n",
    "    pairs = random.sample(pairs, max_pairs_to_use)\n",
    "    print(\"Subsampled pairs to:\", len(pairs))\n",
    "\n",
    "# Create InputExamples (anchor, positive) for MultipleNegativesRankingLoss\n",
    "examples = []\n",
    "for p in pairs:\n",
    "    anchor = p.get(\"anchor\")\n",
    "    positive = p.get(\"positive\")\n",
    "    if not anchor or not positive:\n",
    "        continue\n",
    "    examples.append(InputExample(texts=[anchor, positive]))\n",
    "\n",
    "print(\"Prepared {} InputExample pairs (anchor, positive)\".format(len(examples)))\n",
    "\n",
    "# %% split small dev set for monitoring (sample some pairs)\n",
    "random.shuffle(examples)\n",
    "dev_size = min(512, max(32, int(0.02 * len(examples))))  # 2% or at most 512\n",
    "dev_examples = examples[:dev_size]\n",
    "train_examples = examples[dev_size:]\n",
    "print(\"Train examples:\", len(train_examples), \"Dev examples:\", len(dev_examples))\n",
    "\n",
    "# %% prepare dataloaders & model\n",
    "model = SentenceTransformer(base_model)\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "\n",
    "# evaluator: use EmbeddingSimilarityEvaluator-like approach with dev set\n",
    "# we'll build index of positives and check recall by searching anchor embedding & checking its positive\n",
    "# For convenience, create dev mapping: dev_anchor_text -> dev_positive_text\n",
    "dev_anchor_texts = [ex.texts[0] for ex in dev_examples]\n",
    "dev_positive_texts = [ex.texts[1] for ex in dev_examples]\n",
    "\n",
    "# %%\n",
    "\n",
    "# compute training steps & warmup\n",
    "num_train_steps = int(len(train_dataloader) * num_epochs)\n",
    "warmup_steps = max(1, int(warmup_ratio * num_train_steps))\n",
    "print(f\"Training {num_epochs} epochs, {len(train_dataloader)} steps/epoch, total steps {num_train_steps}, warmup {warmup_steps}\")\n",
    "\n",
    "# Train\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    optimizer_params={\"lr\": learning_rate},\n",
    "    output_path=os.path.join(out_dir, \"mpnet_stage1\"),\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Stage 1 training complete. Model saved at:\", os.path.join(out_dir, \"mpnet_stage1\"))\n",
    "\n",
    "# %% Evaluation: simple Recall@k on dev set using model + FAISS\n",
    "def recall_at_k_for_dev(model, anchor_texts, positive_texts, k=5):\n",
    "    # Encode dev positive texts as \"passage corpus\"\n",
    "    corpus_emb = model.encode(positive_texts, convert_to_numpy=True, batch_size=128, show_progress_bar=False)\n",
    "    query_emb = model.encode(anchor_texts, convert_to_numpy=True, batch_size=128, show_progress_bar=False)\n",
    "    # normalize for IP\n",
    "    faiss.normalize_L2(corpus_emb)\n",
    "    faiss.normalize_L2(query_emb)\n",
    "    d = corpus_emb.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(corpus_emb)\n",
    "    D, I = index.search(query_emb, k)\n",
    "    hits = 0\n",
    "    for qi, inds in enumerate(I):\n",
    "        # check if the gold positive (index qi) is among retrieved (note we built corpus of positives in same order)\n",
    "        # this simplistic check requires that the corpus ordering matches dev positives; since we encoded positives in that order, gold index is qi\n",
    "        if qi in inds:\n",
    "            hits += 1\n",
    "    return hits / len(anchor_texts)\n",
    "\n",
    "if faiss is not None and len(dev_anchor_texts) > 0:\n",
    "    r1 = recall_at_k_for_dev(model, dev_anchor_texts, dev_positive_texts, k=1)\n",
    "    r5 = recall_at_k_for_dev(model, dev_anchor_texts, dev_positive_texts, k=5)\n",
    "    print(f\"Dev Recall@1 = {r1:.4f}, Recall@5 = {r5:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping dev Recall@k (faiss not available or no dev examples).\")\n",
    "\n",
    "# %% Save initial model & (optional) encode and save passages embeddings if provided\n",
    "passage_texts = []\n",
    "if os.path.exists(passages_path):\n",
    "    passages = read_jsonl(passages_path)\n",
    "    passage_texts = [p.get(\"text\") for p in passages if p.get(\"text\")]\n",
    "else:\n",
    "    print(\"No passages file found at\", passages_path)\n",
    "save_model_and_embeddings(model, passage_texts, out_dir=os.path.join(out_dir, \"mpnet_stage1\"))\n",
    "\n",
    "# %% Optional: Hard-negative mining & retrain with BatchHardTripletLoss\n",
    "if do_hard_negative_mining and faiss is not None:\n",
    "    print(\"Starting hard negative mining using FAISS (top_k =\", mine_top_k, \") ...\")\n",
    "    # Build index over passages_texts (if not available, we mine from positives in train pairs)\n",
    "    if len(passage_texts) == 0:\n",
    "        # fallback: build pool from positive texts in pairs\n",
    "        pool_texts = [p['positive'] for p in pairs if p.get('positive')]\n",
    "    else:\n",
    "        pool_texts = passage_texts\n",
    "\n",
    "    pool_emb = model.encode(pool_texts, convert_to_numpy=True, batch_size=256, show_progress_bar=True)\n",
    "    faiss.normalize_L2(pool_emb)\n",
    "    d = pool_emb.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(pool_emb)\n",
    "\n",
    "    # For each anchor, retrieve top_k and pick negatives (skip identical/overlapping by exact text match)\n",
    "    hard_triplets = []\n",
    "    anchors = [p['anchor'] for p in pairs if p.get('anchor') and p.get('positive')]\n",
    "    positives = [p['positive'] for p in pairs if p.get('anchor') and p.get('positive')]\n",
    "\n",
    "    # For efficiency, sample a subset for mining if dataset large\n",
    "    max_mine = min(len(anchors), 20000)  # cap mining to 20k anchors for speed; adjust if you like\n",
    "    sample_idx = random.sample(range(len(anchors)), max_mine)\n",
    "\n",
    "    for idx in tqdm(sample_idx):\n",
    "        a_text = anchors[idx]\n",
    "        pos_text = positives[idx]\n",
    "        q_emb = model.encode(a_text, convert_to_numpy=True)\n",
    "        faiss.normalize_L2(q_emb)\n",
    "        D, I = index.search(q_emb.reshape(1,-1), mine_top_k)\n",
    "        # iterate candidates and pick hard negatives that are not the positive and not equal to anchor\n",
    "        picked = 0\n",
    "        for cand_i in I[0]:\n",
    "            cand_text = pool_texts[cand_i]\n",
    "            if cand_text == pos_text or cand_text == a_text:\n",
    "                continue\n",
    "            # optionally ensure negative from other chapter by checking meta — skipped here for simplicity\n",
    "            hard_triplets.append((a_text, pos_text, cand_text))\n",
    "            picked += 1\n",
    "            if picked >= hard_negatives_per_anchor:\n",
    "                break\n",
    "\n",
    "    print(\"Mined hard triplets:\", len(hard_triplets))\n",
    "\n",
    "    # Convert to InputExamples for BatchHardTripletLoss (texts=[anchor, pos, neg])\n",
    "    triplet_examples = [InputExample(texts=[a,p,n]) for (a,p,n) in hard_triplets]\n",
    "\n",
    "    # dataloader and loss\n",
    "    triplet_dataloader = DataLoader(triplet_examples, shuffle=True, batch_size=max(16, batch_size//4))\n",
    "    triplet_loss = losses.BatchHardTripletLoss(model=model, distance_metric=losses.BatchHardTripletLoss.distance_cosine, margin=0.2)\n",
    "\n",
    "    # retrain\n",
    "    num_steps = int(len(triplet_dataloader) * hard_retrain_epochs)\n",
    "    warmup_steps = max(1, int(0.1 * num_steps))\n",
    "    model.fit(\n",
    "        train_objectives=[(triplet_dataloader, triplet_loss)],\n",
    "        epochs=hard_retrain_epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        optimizer_params={\"lr\": learning_rate},\n",
    "        output_path=os.path.join(out_dir, \"mpnet_hardneg\"),\n",
    "        show_progress_bar=True,\n",
    "        num_items_in_batch=None\n",
    "    )\n",
    "    print(\"Hard-negative retrain complete. Saved at\", os.path.join(out_dir, \"mpnet_hardneg\"))\n",
    "    # save embeddings again\n",
    "    save_model_and_embeddings(model, passage_texts, out_dir=os.path.join(out_dir, \"mpnet_hardneg\"))\n",
    "\n",
    "else:\n",
    "    print(\"Skipping hard-negative mining (faiss not available or disabled).\")\n",
    "\n",
    "# %% Done\n",
    "print(\"All done. Models stored under:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d5aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "Contrastive fine-tuning notebook for multi-qa-mpnet on Mahabharata dataset.\n",
    "Inputs required:\n",
    " - train_pairs.jsonl : lines of {\"anchor\",\"positive\",\"negatives\",\"meta\"}\n",
    " - passages.jsonl (optional) : passages with \"text\" fields (for indexing/eval)\n",
    "Outputs:\n",
    " - saved model under out_dir (default ./mpnet_ft)\n",
    " - passage embeddings: out_dir/passage_embeddings.npy\n",
    " - faiss index: out_dir/faiss_index.idx\n",
    "\"\"\"\n",
    "# %% imports\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# huggingface / sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, util, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# optional faiss (install faiss-cpu or faiss-gpu)\n",
    "try:\n",
    "    import faiss\n",
    "except Exception:\n",
    "    faiss = None\n",
    "    print(\"faiss not available. Hard-negative mining will be skipped unless you install faiss-cpu or faiss-gpu.\")\n",
    "\n",
    "# %% user params - adjust as needed\n",
    "train_pairs_path = \"./dataset_out_simple/train_pairs_clean.jsonl\"  # path to your train pairs\n",
    "passages_path = \"./dataset_out_simple/passages_clean.jsonl\"        # optional, used for indexing\n",
    "out_dir = \"./mpnet_ft\"\n",
    "base_model = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "\n",
    "# training hyperparams\n",
    "batch_size = 64           # try 64, increase if GPU allows\n",
    "num_epochs = 2               # 1-3 is usually fine for contrastive\n",
    "learning_rate = 2e-5\n",
    "max_pairs_to_use = None      # set int to subsample, or None to use all\n",
    "warmup_ratio = 0.1           # proportion of steps for warmup\n",
    "\n",
    "# hard-negative mining params\n",
    "do_hard_negative_mining = True if faiss is not None else False\n",
    "mine_top_k = 64\n",
    "hard_negatives_per_anchor = 2\n",
    "hard_retrain_epochs = 1\n",
    "\n",
    "# device/mixed precision handled by sentence-transformers automatically if installed with pytorch+cuda\n",
    "\n",
    "# %% helper functions\n",
    "def read_jsonl(path):\n",
    "    items = []\n",
    "    with open(path, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            items.append(json.loads(line))\n",
    "    return items\n",
    "\n",
    "def save_model_and_embeddings(model, passages_texts, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    model.save(out_dir)\n",
    "    print(\"Saved model to\", out_dir)\n",
    "    if passages_texts is not None and len(passages_texts) > 0:\n",
    "        emb = model.encode(passages_texts, batch_size=256, convert_to_numpy=True, show_progress_bar=True)\n",
    "        np.save(os.path.join(out_dir, \"passage_embeddings.npy\"), emb)\n",
    "        print(\"Saved passage embeddings (shape {})\".format(emb.shape))\n",
    "        if faiss is not None:\n",
    "            # normalize for inner product search\n",
    "            faiss.normalize_L2(emb)\n",
    "            d = emb.shape[1]\n",
    "            index = faiss.IndexFlatIP(d)\n",
    "            index.add(emb)\n",
    "            faiss.write_index(index, os.path.join(out_dir, \"faiss_index.idx\"))\n",
    "            print(\"Built & saved FAISS index at\", os.path.join(out_dir, \"faiss_index.idx\"))\n",
    "    else:\n",
    "        print(\"No passages provided; skipping embedding/index save.\")\n",
    "\n",
    "# %% Load training pairs\n",
    "print(\"Loading train pairs from:\", train_pairs_path)\n",
    "pairs = read_jsonl(train_pairs_path)\n",
    "print(\"Total raw pairs:\", len(pairs))\n",
    "if max_pairs_to_use is not None and max_pairs_to_use < len(pairs):\n",
    "    pairs = random.sample(pairs, max_pairs_to_use)\n",
    "    print(\"Subsampled pairs to:\", len(pairs))\n",
    "\n",
    "# Create InputExamples (anchor, positive) for MultipleNegativesRankingLoss\n",
    "examples = []\n",
    "for p in pairs:\n",
    "    anchor = p.get(\"anchor\")\n",
    "    positive = p.get(\"positive\")\n",
    "    if not anchor or not positive:\n",
    "        continue\n",
    "    examples.append(InputExample(texts=[anchor, positive]))\n",
    "\n",
    "print(\"Prepared {} InputExample pairs (anchor, positive)\".format(len(examples)))\n",
    "\n",
    "# %% split small dev set for monitoring (sample some pairs)\n",
    "random.shuffle(examples)\n",
    "dev_size = min(512, max(32, int(0.02 * len(examples))))  # 2% or at most 512\n",
    "dev_examples = examples[:dev_size]\n",
    "train_examples = examples[dev_size:]\n",
    "print(\"Train examples:\", len(train_examples), \"Dev examples:\", len(dev_examples))\n",
    "\n",
    "# %% prepare dataloaders & model\n",
    "model = SentenceTransformer(base_model)\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "\n",
    "# evaluator: use EmbeddingSimilarityEvaluator-like approach with dev set\n",
    "# we'll build index of positives and check recall by searching anchor embedding & checking its positive\n",
    "# For convenience, create dev mapping: dev_anchor_text -> dev_positive_text\n",
    "dev_anchor_texts = [ex.texts[0] for ex in dev_examples]\n",
    "dev_positive_texts = [ex.texts[1] for ex in dev_examples]\n",
    "\n",
    "# %%\n",
    "\n",
    "# compute training steps & warmup\n",
    "num_train_steps = int(len(train_dataloader) * num_epochs)\n",
    "warmup_steps = max(1, int(warmup_ratio * num_train_steps))\n",
    "print(f\"Training {num_epochs} epochs, {len(train_dataloader)} steps/epoch, total steps {num_train_steps}, warmup {warmup_steps}\")\n",
    "\n",
    "# Train\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    optimizer_params={\"lr\": learning_rate},\n",
    "    output_path=os.path.join(out_dir, \"mpnet_stage1\"),\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Stage 1 training complete. Model saved at:\", os.path.join(out_dir, \"mpnet_stage1\"))\n",
    "\n",
    "# %% Evaluation: simple Recall@k on dev set using model + FAISS\n",
    "def recall_at_k_for_dev(model, anchor_texts, positive_texts, k=5):\n",
    "    # Encode dev positive texts as \"passage corpus\"\n",
    "    corpus_emb = model.encode(positive_texts, convert_to_numpy=True, batch_size=128, show_progress_bar=False)\n",
    "    query_emb = model.encode(anchor_texts, convert_to_numpy=True, batch_size=128, show_progress_bar=False)\n",
    "    # normalize for IP\n",
    "    faiss.normalize_L2(corpus_emb)\n",
    "    faiss.normalize_L2(query_emb)\n",
    "    d = corpus_emb.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(corpus_emb)\n",
    "    D, I = index.search(query_emb, k)\n",
    "    hits = 0\n",
    "    for qi, inds in enumerate(I):\n",
    "        # check if the gold positive (index qi) is among retrieved (note we built corpus of positives in same order)\n",
    "        # this simplistic check requires that the corpus ordering matches dev positives; since we encoded positives in that order, gold index is qi\n",
    "        if qi in inds:\n",
    "            hits += 1\n",
    "    return hits / len(anchor_texts)\n",
    "\n",
    "if faiss is not None and len(dev_anchor_texts) > 0:\n",
    "    r1 = recall_at_k_for_dev(model, dev_anchor_texts, dev_positive_texts, k=1)\n",
    "    r5 = recall_at_k_for_dev(model, dev_anchor_texts, dev_positive_texts, k=5)\n",
    "    print(f\"Dev Recall@1 = {r1:.4f}, Recall@5 = {r5:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping dev Recall@k (faiss not available or no dev examples).\")\n",
    "\n",
    "# %% Save initial model & (optional) encode and save passages embeddings if provided\n",
    "passage_texts = []\n",
    "if os.path.exists(passages_path):\n",
    "    passages = read_jsonl(passages_path)\n",
    "    passage_texts = [p.get(\"text\") for p in passages if p.get(\"text\")]\n",
    "else:\n",
    "    print(\"No passages file found at\", passages_path)\n",
    "save_model_and_embeddings(model, passage_texts, out_dir=os.path.join(out_dir, \"mpnet_stage1\"))\n",
    "\n",
    "# %% Optional: Hard-negative mining & retrain with BatchHardTripletLoss\n",
    "if do_hard_negative_mining and faiss is not None:\n",
    "    print(\"Starting hard negative mining using FAISS (top_k =\", mine_top_k, \") ...\")\n",
    "    # Build index over passages_texts (if not available, we mine from positives in train pairs)\n",
    "    if len(passage_texts) == 0:\n",
    "        # fallback: build pool from positive texts in pairs\n",
    "        pool_texts = [p['positive'] for p in pairs if p.get('positive')]\n",
    "    else:\n",
    "        pool_texts = passage_texts\n",
    "\n",
    "    pool_emb = model.encode(pool_texts, convert_to_numpy=True, batch_size=256, show_progress_bar=True)\n",
    "    faiss.normalize_L2(pool_emb)\n",
    "    d = pool_emb.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(pool_emb)\n",
    "\n",
    "    # For each anchor, retrieve top_k and pick negatives (skip identical/overlapping by exact text match)\n",
    "    hard_triplets = []\n",
    "    anchors = [p['anchor'] for p in pairs if p.get('anchor') and p.get('positive')]\n",
    "    positives = [p['positive'] for p in pairs if p.get('anchor') and p.get('positive')]\n",
    "\n",
    "    # For efficiency, sample a subset for mining if dataset large\n",
    "    max_mine = min(len(anchors), 20000)  # cap mining to 20k anchors for speed; adjust if you like\n",
    "    sample_idx = random.sample(range(len(anchors)), max_mine)\n",
    "\n",
    "    for idx in tqdm(sample_idx):\n",
    "        a_text = anchors[idx]\n",
    "        pos_text = positives[idx]\n",
    "        q_emb = model.encode(a_text, convert_to_numpy=True)\n",
    "        faiss.normalize_L2(q_emb)\n",
    "        D, I = index.search(q_emb.reshape(1,-1), mine_top_k)\n",
    "        # iterate candidates and pick hard negatives that are not the positive and not equal to anchor\n",
    "        picked = 0\n",
    "        for cand_i in I[0]:\n",
    "            cand_text = pool_texts[cand_i]\n",
    "            if cand_text == pos_text or cand_text == a_text:\n",
    "                continue\n",
    "            # optionally ensure negative from other chapter by checking meta — skipped here for simplicity\n",
    "            hard_triplets.append((a_text, pos_text, cand_text))\n",
    "            picked += 1\n",
    "            if picked >= hard_negatives_per_anchor:\n",
    "                break\n",
    "\n",
    "    print(\"Mined hard triplets:\", len(hard_triplets))\n",
    "\n",
    "    # Convert to InputExamples for BatchHardTripletLoss (texts=[anchor, pos, neg])\n",
    "    triplet_examples = [InputExample(texts=[a,p,n]) for (a,p,n) in hard_triplets]\n",
    "\n",
    "    # dataloader and loss\n",
    "    triplet_dataloader = DataLoader(triplet_examples, shuffle=True, batch_size=max(16, batch_size//4))\n",
    "    triplet_loss = losses.BatchHardTripletLoss(model=model, distance_metric=losses.BatchHardTripletLoss.distance_cosine, margin=0.2)\n",
    "\n",
    "    # retrain\n",
    "    num_steps = int(len(triplet_dataloader) * hard_retrain_epochs)\n",
    "    warmup_steps = max(1, int(0.1 * num_steps))\n",
    "    model.fit(\n",
    "        train_objectives=[(triplet_dataloader, triplet_loss)],\n",
    "        epochs=hard_retrain_epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        optimizer_params={\"lr\": learning_rate},\n",
    "        output_path=os.path.join(out_dir, \"mpnet_hardneg\"),\n",
    "        show_progress_bar=True,\n",
    "        num_items_in_batch=None\n",
    "    )\n",
    "    print(\"Hard-negative retrain complete. Saved at\", os.path.join(out_dir, \"mpnet_hardneg\"))\n",
    "    # save embeddings again\n",
    "    save_model_and_embeddings(model, passage_texts, out_dir=os.path.join(out_dir, \"mpnet_hardneg\"))\n",
    "\n",
    "else:\n",
    "    print(\"Skipping hard-negative mining (faiss not available or disabled).\")\n",
    "\n",
    "# %% Done\n",
    "print(\"All done. Models stored under:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37159dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda batch_size: 16 use_amp: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44448/2573917917.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if (use_amp and device.type == \"cuda\") else None\n",
      "Epoch 1/2:   0%|          | 0/1843 [00:00<?, ?it/s]/tmp/ipykernel_44448/2573917917.py:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/2: 100%|██████████| 1843/1843 [27:19<00:00,  1.12it/s, loss=0.464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done — avg loss: 0.4642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 1843/1843 [27:24<00:00,  1.12it/s, loss=0.138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 done — avg loss: 0.1384\n",
      "Training finished. Checkpoints in ./mpnet_manual_ft\n"
     ]
    }
   ],
   "source": [
    "# Manual contrastive training loop (safe & robust) - paste and run\n",
    "import os, random, gc, torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# CONFIG\n",
    "base_model = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 16          # lower if OOM (try 2 or 1)\n",
    "accumulation_steps = 1\n",
    "num_epochs = 2\n",
    "learning_rate = 2e-5\n",
    "max_length = 256          # reduce if inputs are long\n",
    "temperature = .05\n",
    "out_dir = \"./mpnet_manual_ft\"\n",
    "use_amp = True if device.type == \"cuda\" else False\n",
    "\n",
    "print(\"Device:\", device, \"batch_size:\", batch_size, \"use_amp:\", use_amp)\n",
    "\n",
    "# Prepare tokenizer + model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "encoder = AutoModel.from_pretrained(base_model).to(device)\n",
    "encoder.train()\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    summed = torch.sum(token_embeddings * mask, dim=1)\n",
    "    counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, pairs): self.pairs = pairs\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.pairs[idx]; return p['anchor'], p['positive']\n",
    "\n",
    "# shuffle and create dataloader\n",
    "random.shuffle(pairs)\n",
    "dataset = PairDataset(pairs)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0)\n",
    "\n",
    "optimizer = optim.AdamW(encoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = torch.cuda.amp.GradScaler() if (use_amp and device.type == \"cuda\") else None\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    total_loss = 0.0\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    optimizer.zero_grad()\n",
    "    for step, (anchors, positives) in enumerate(pbar):\n",
    "        encoded_a = tokenizer(list(anchors), padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length).to(device)\n",
    "        encoded_p = tokenizer(list(positives), padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length).to(device)\n",
    "\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out_a = encoder(**encoded_a)\n",
    "                out_p = encoder(**encoded_p)\n",
    "                emb_a = mean_pooling(out_a, encoded_a['attention_mask'])\n",
    "                emb_p = mean_pooling(out_p, encoded_p['attention_mask'])\n",
    "                emb_a = nn.functional.normalize(emb_a, p=2, dim=1)\n",
    "                emb_p = nn.functional.normalize(emb_p, p=2, dim=1)\n",
    "                logits = torch.matmul(emb_a, emb_p.t()) / temperature\n",
    "                labels = torch.arange(logits.size(0), device=logits.device)\n",
    "                loss = criterion(logits, labels) / accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            out_a = encoder(**encoded_a)\n",
    "            out_p = encoder(**encoded_p)\n",
    "            emb_a = mean_pooling(out_a, encoded_a['attention_mask'])\n",
    "            emb_p = mean_pooling(out_p, encoded_p['attention_mask'])\n",
    "            emb_a = nn.functional.normalize(emb_a, p=2, dim=1)\n",
    "            emb_p = nn.functional.normalize(emb_p, p=2, dim=1)\n",
    "            logits = torch.matmul(emb_a, emb_p.t()) / temperature\n",
    "            labels = torch.arange(logits.size(0), device=logits.device)\n",
    "            loss = criterion(logits, labels) / accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            if scaler is not None:\n",
    "                scaler.step(optimizer); scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += (loss.item() * accumulation_steps)\n",
    "        pbar.set_postfix({'loss': total_loss / (step+1)})\n",
    "\n",
    "    avg_loss = total_loss / (step+1)\n",
    "    print(f\"Epoch {epoch+1} done — avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    ck = os.path.join(out_dir, f\"epoch_{epoch+1}\")\n",
    "    os.makedirs(ck, exist_ok=True)\n",
    "    encoder.save_pretrained(ck); tokenizer.save_pretrained(ck)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "print(\"Training finished. Checkpoints in\", out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81648158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval device: cuda\n",
      "Dev Recall@K: {1: 0.533, 5: 0.912, 10: 0.973} MRR: 0.696156349206349\n",
      "No gradients present right now. (Run one backward or check training loop)\n"
     ]
    }
   ],
   "source": [
    "# ===== Evaluate dev Recall@K & check grads =====\n",
    "import numpy as np, torch, gc\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "# 1) Quick eval function (uses in-memory dev_pairs)\n",
    "def encode_texts(texts, tokenizer, model, device='cuda', batch_size=32, max_length=256):\n",
    "    model.eval()\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            encoded = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length).to(device)\n",
    "            out = model(**encoded)\n",
    "            emb = out.last_hidden_state.mean(dim=1)\n",
    "            emb = normalize(emb, p=2, dim=1)\n",
    "            embs.append(emb.cpu().numpy())\n",
    "    return np.vstack(embs)\n",
    "\n",
    "# prepare dev set (if not already)\n",
    "dev_pairs = dev_pairs if 'dev_pairs' in globals() else pairs[:min(1000, max(200, int(0.05*len(pairs))))]\n",
    "dev_anchors = [p['anchor'] for p in dev_pairs]\n",
    "dev_positives = [p['positive'] for p in dev_pairs]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Eval device:\", device)\n",
    "\n",
    "# encode dev anchors & positives (corpus == positives here for simplicity)\n",
    "q_embs = encode_texts(dev_anchors, tokenizer, encoder, device=device, batch_size=32, max_length=256)\n",
    "p_embs = encode_texts(dev_positives, tokenizer, encoder, device=device, batch_size=64, max_length=256)\n",
    "\n",
    "# simple CPU retrieval\n",
    "sims = q_embs @ p_embs.T   # (Q, N)\n",
    "topk = np.argsort(-sims, axis=1)[:, :10]\n",
    "\n",
    "# compute Recall@K and MRR\n",
    "K_list = [1,5,10]\n",
    "Q = len(q_embs)\n",
    "recalls = {k:0 for k in K_list}\n",
    "mrr = 0.0\n",
    "for i in range(Q):\n",
    "    gold = i   # since dev_positives are aligned\n",
    "    ranks = np.where(topk[i]==gold)[0]\n",
    "    for k in K_list:\n",
    "        if gold in topk[i,:k]:\n",
    "            recalls[k]+=1\n",
    "    if ranks.size>0:\n",
    "        mrr += 1.0/(ranks[0]+1)\n",
    "mrr /= Q\n",
    "recalls = {k: recalls[k]/Q for k in K_list}\n",
    "print(\"Dev Recall@K:\", recalls, \"MRR:\", mrr)\n",
    "\n",
    "# 2) Check gradient norms (run after a backward step or on last step)\n",
    "grads = [p.grad.norm().item() for p in encoder.parameters() if p.grad is not None]\n",
    "if len(grads)>0:\n",
    "    print(\"Grad stats: mean {:.3e}, max {:.3e}, min {:.3e}\".format(np.mean(grads), np.max(grads), np.min(grads)))\n",
    "else:\n",
    "    print(\"No gradients present right now. (Run one backward or check training loop)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
