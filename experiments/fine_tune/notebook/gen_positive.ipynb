{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca53835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;34musage: \u001b[0m\u001b[1;35mipykernel_launcher.py\u001b[0m [\u001b[32m-h\u001b[0m] [\u001b[36m--purva_dir \u001b[33mPURVA_DIR\u001b[0m] [\u001b[36m--out_dir \u001b[33mOUT_DIR\u001b[0m]\n",
      "                             [\u001b[36m--window_sizes \u001b[33mWINDOW_SIZES [WINDOW_SIZES ...]\u001b[0m]\n",
      "                             [\u001b[36m--overlaps \u001b[33mOVERLAPS [OVERLAPS ...]\u001b[0m]\n",
      "                             [\u001b[36m--max_pairs \u001b[33mMAX_PAIRS\u001b[0m] [\u001b[36m--seed \u001b[33mSEED\u001b[0m]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/roopeshmangal/Library/Jupyter/runtime/kernel-v39c55b277988392caeabe0b098721424ff50f1676.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roopeshmangal/Downloads/iitk/cs787/venv/lib/python3.14/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "create_dataset_mahabharata.py\n",
    "\n",
    "This script only creates the dataset needed for contrastive training.\n",
    "It reads Purva files (JSONL with verse_id and text), builds multi-scale\n",
    "sliding windows, and outputs two files:\n",
    "  - passages.jsonl : each line is a JSON with passage metadata and text\n",
    "  - train_pairs.jsonl : each line is a JSON {\"anchor\":..., \"positive\":..., \"meta\":...}\n",
    "\n",
    "Usage:\n",
    "  python create_dataset_mahabharata.py --purva_dir ./purvas --out_dir ./dataset_out \\\n",
    "        --window_sizes 16 32 48 --overlaps 8 16 24 --max_pairs 50000\n",
    "\n",
    "Requirements:\n",
    "  pip install tqdm\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "def load_purvas(purva_dir):\n",
    "    purvas = {}\n",
    "    for f in sorted(Path(purva_dir).glob(\"*.jsonl\")):\n",
    "        purva_name = f.stem\n",
    "        verses = []\n",
    "        with f.open(encoding=\"utf8\") as fh:\n",
    "            for line in fh:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    v = json.loads(line)\n",
    "                except Exception:\n",
    "                    parts = line.split(\"\t\", 1)\n",
    "                    v = {\"verse_id\": parts[0].strip(), \"text\": parts[1].strip() if len(parts) > 1 else \"\"}\n",
    "                verses.append(v)\n",
    "        purvas[purva_name] = verses\n",
    "    return purvas\n",
    "\n",
    "\n",
    "def build_windows(verses, window_size, overlap):\n",
    "    stride = max(1, window_size - overlap)\n",
    "    windows = []\n",
    "    n = len(verses)\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        start = i\n",
    "        end = min(i + window_size, n)\n",
    "        block = verses[start:end]\n",
    "        text = \"\".join([f\"{v['verse_id']} {v['text']}\" for v in block])\n",
    "        windows.append({\n",
    "            \"start_idx\": start,\n",
    "            \"end_idx\": end - 1,\n",
    "            \"start_verse\": block[0][\"verse_id\"],\n",
    "            \"end_verse\": block[-1][\"verse_id\"],\n",
    "            \"text\": text,\n",
    "            \"verse_count\": len(block)\n",
    "        })\n",
    "        if end == n:\n",
    "            break\n",
    "        i += stride\n",
    "    return windows\n",
    "\n",
    "\n",
    "def create_positive_pairs_for_purva(windows_by_size, max_shifts=2):\n",
    "    pairs = []\n",
    "    sizes = sorted(windows_by_size.keys())\n",
    "    for W in sizes:\n",
    "        windows = windows_by_size[W]\n",
    "        for i in range(len(windows)):\n",
    "            a = windows[i][\"text\"]\n",
    "            # adjacent\n",
    "            if i + 1 < len(windows):\n",
    "                pairs.append((a, windows[i + 1][\"text\"], {\"size\": W, \"type\": \"adjacent\", \"idx\": i}))\n",
    "            # shifts\n",
    "            for s in range(1, max_shifts + 1):\n",
    "                if i + s + 1 < len(windows):\n",
    "                    pairs.append((a, windows[i + s + 1][\"text\"], {\"size\": W, \"type\": f\"shift{s+1}\", \"idx\": i}))\n",
    "    # multi-scale: pair smaller windows to larger windows that overlap\n",
    "    for i, W in enumerate(sizes):\n",
    "        for L in sizes[i+1:]:\n",
    "            small_ws = windows_by_size[W]\n",
    "            large_ws = windows_by_size[L]\n",
    "            # naive overlap: map by proportional index\n",
    "            for idx_s, sw in enumerate(small_ws):\n",
    "                j = min(len(large_ws)-1, int(idx_s * (len(large_ws) / max(1, len(small_ws)))))\n",
    "                pairs.append((sw[\"text\"], large_ws[j][\"text\"], {\"size\": W, \"type\": \"multiscale\", \"larger\": L, \"idx\": idx_s}))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def build_dataset(purvas, window_sizes, overlaps, max_pairs=None, shuffle=True):\n",
    "    all_passages = []\n",
    "    train_pairs = []\n",
    "    purva_meta = {}\n",
    "\n",
    "    for purva_name, verses in purvas.items():\n",
    "        windows_by_size = {}\n",
    "        for W, O in zip(window_sizes, overlaps):\n",
    "            windows_by_size[W] = build_windows(verses, W, O)\n",
    "        # flatten windows into all_passages and keep mapping\n",
    "        start_idx = len(all_passages)\n",
    "        for W in window_sizes:\n",
    "            for w in windows_by_size[W]:\n",
    "                w_meta = dict(w)\n",
    "                w_meta.update({\"purva\": purva_name, \"window_size\": W})\n",
    "                all_passages.append(w_meta)\n",
    "        end_idx = len(all_passages) - 1\n",
    "        purva_meta[purva_name] = {\"start_idx\": start_idx, \"end_idx\": end_idx, \"counts\": {W: len(windows_by_size[W]) for W in window_sizes}}\n",
    "\n",
    "        # create positive pairs within this purva\n",
    "        pairs = create_positive_pairs_for_purva(windows_by_size)\n",
    "        # attach purva name in meta\n",
    "        for a, b, m in pairs:\n",
    "            m = dict(m)\n",
    "            m.update({\"purva\": purva_name})\n",
    "            train_pairs.append({\"anchor\": a, \"positive\": b, \"meta\": m})\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(train_pairs)\n",
    "    if max_pairs and max_pairs < len(train_pairs):\n",
    "        train_pairs = train_pairs[:max_pairs]\n",
    "    return all_passages, train_pairs, purva_meta\n",
    "\n",
    "\n",
    "def save_jsonl(items, out_path):\n",
    "    with open(out_path, 'w', encoding='utf8') as fh:\n",
    "        for it in items:\n",
    "            fh.write(json.dumps(it, ensure_ascii=False) + '')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--purva_dir', type=str, default='./purvas', help='Directory of purva jsonl files')\n",
    "    parser.add_argument('--out_dir', type=str, default='./dataset_out', help='Where to save passages.jsonl and train_pairs.jsonl')\n",
    "    parser.add_argument('--window_sizes', type=int, nargs='+', default=[16,32,48])\n",
    "    parser.add_argument('--overlaps', type=int, nargs='+', default=[8,16,24])\n",
    "    parser.add_argument('--max_pairs', type=int, default=None)\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "\n",
    "    purvas = load_purvas(args.purva_dir)\n",
    "    print(f\"Loaded purvas: {list(purvas.keys())}\")\n",
    "\n",
    "    passages, train_pairs, purva_meta = build_dataset(purvas, args.window_sizes, args.overlaps, max_pairs=args.max_pairs)\n",
    "\n",
    "    print(f\"Built {len(passages)} passages and {len(train_pairs)} train pairs\")\n",
    "\n",
    "    # Save passages (with metadata) and train pairs\n",
    "    save_jsonl(passages, os.path.join(args.out_dir, 'passages.jsonl'))\n",
    "    save_jsonl(train_pairs, os.path.join(args.out_dir, 'train_pairs.jsonl'))\n",
    "\n",
    "    # Save purva meta\n",
    "    with open(os.path.join(args.out_dir, 'purva_meta.json'), 'w', encoding='utf8') as fh:\n",
    "        json.dump(purva_meta, fh, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print('Saved dataset files in', args.out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7acdd",
   "metadata": {},
   "source": [
    "### generate positve , anchor,nagtive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "Simple dataset builder for contrastive training (no ids â€” raw texts only).\n",
    "\n",
    "Outputs (out_dir):\n",
    " - passages.jsonl       : all passage windows (with text + minimal meta)\n",
    " - train_pairs.jsonl    : each line {\"anchor\":..., \"positive\":..., \"negatives\":[...], \"meta\":...}\n",
    " - preview_pairs.jsonl  : first 20 pairs (same format) for inspection\n",
    "\n",
    "Usage:\n",
    " - Put your chapter dictionary into the variable `chapters_dict` below (chapter_id -> list of verses).\n",
    " - Run all cells.\n",
    "\"\"\"\n",
    "# %% imports\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# %% utils\n",
    "def ensure_dir(d: str):\n",
    "    Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_jsonl(items: List[Dict[str, Any]], path: str):\n",
    "    with open(path, 'w', encoding='utf8') as fh:\n",
    "        for it in items:\n",
    "            fh.write(json.dumps(it, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# %% normalize input chapters\n",
    "def normalize_chapters_from_dict(chapters_raw: Dict[str, List[Any]]) -> Dict[str, List[Dict[str,str]]]:\n",
    "    \"\"\"\n",
    "    Accepts chapter_id -> list of verses (strings or dicts).\n",
    "    Returns chapter_id -> list of {\"verse_id\",\"text\"}.\n",
    "    \"\"\"\n",
    "    chapters = {}\n",
    "    for ch_id, verses in chapters_raw.items():\n",
    "        parsed = []\n",
    "        for i, v in enumerate(verses):\n",
    "            if isinstance(v, str):\n",
    "                verse_id = f\"{ch_id}.{i+1:03d}\"\n",
    "                parsed.append({\"verse_id\": verse_id, \"text\": v})\n",
    "            elif isinstance(v, dict):\n",
    "                vid = v.get(\"verse_id\") or f\"{ch_id}.{i+1:03d}\"\n",
    "                txt = v.get(\"text\") or v.get(\"verse\") or \"\"\n",
    "                parsed.append({\"verse_id\": vid, \"text\": txt})\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported verse format in chapter {ch_id}: {v}\")\n",
    "        chapters[ch_id] = parsed\n",
    "    return chapters\n",
    "\n",
    "# %% build windows (within chapter)\n",
    "def build_windows_for_chapter(verses: List[Dict[str, str]], window_size: int, overlap: int) -> List[Dict[str, Any]]:\n",
    "    stride = max(1, window_size - overlap)\n",
    "    windows = []\n",
    "    n = len(verses)\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        start = i\n",
    "        end = min(i + window_size, n)\n",
    "        block = verses[start:end]\n",
    "        text = \"\\n\".join([f\"{v['verse_id']} {v['text']}\" for v in block])\n",
    "        windows.append({\n",
    "            \"chapter\": None,          # filled later\n",
    "            \"start_idx\": start,\n",
    "            \"end_idx\": end - 1,\n",
    "            \"start_verse\": block[0][\"verse_id\"],\n",
    "            \"end_verse\": block[-1][\"verse_id\"],\n",
    "            \"text\": text,\n",
    "            \"verse_count\": len(block)\n",
    "        })\n",
    "        if end == n:\n",
    "            break\n",
    "        i += stride\n",
    "    return windows\n",
    "\n",
    "# %% create pairs (anchor/positive by text)\n",
    "def create_positive_pairs_text(windows_by_size: Dict[int, List[Dict[str, Any]]], max_shifts: int = 2) -> List[Dict[str, Any]]:\n",
    "    pairs = []\n",
    "    sizes = sorted(windows_by_size.keys())\n",
    "    for W in sizes:\n",
    "        windows = windows_by_size[W]\n",
    "        for i in range(len(windows)):\n",
    "            a_text = windows[i][\"text\"]\n",
    "            # adjacent\n",
    "            if i + 1 < len(windows):\n",
    "                pairs.append({\"anchor\": a_text, \"positive\": windows[i + 1][\"text\"], \"meta\": {\"size\": W, \"type\": \"adjacent\", \"idx\": i}})\n",
    "            # shifts\n",
    "            for s in range(1, max_shifts + 1):\n",
    "                if i + s + 1 < len(windows):\n",
    "                    pairs.append({\"anchor\": a_text, \"positive\": windows[i + s + 1][\"text\"], \"meta\": {\"size\": W, \"type\": f\"shift{s+1}\", \"idx\": i}})\n",
    "    # multi-scale: small -> larger overlapping (by text)\n",
    "    for i, W in enumerate(sizes):\n",
    "        for L in sizes[i+1:]:\n",
    "            small_ws = windows_by_size[W]\n",
    "            large_ws = windows_by_size[L]\n",
    "            if not small_ws or not large_ws:\n",
    "                continue\n",
    "            for idx_s, sw in enumerate(small_ws):\n",
    "                j = min(len(large_ws)-1, int(idx_s * (len(large_ws) / max(1, len(small_ws)))))\n",
    "                pairs.append({\"anchor\": sw[\"text\"], \"positive\": large_ws[j][\"text\"], \"meta\": {\"size\": W, \"type\": \"multiscale\", \"larger\": L, \"idx\": idx_s}})\n",
    "    return pairs\n",
    "\n",
    "# %% main builder (simplified: stores full texts)\n",
    "def build_simple_dataset(\n",
    "    chapters_raw: Dict[str, List[Any]],\n",
    "    window_sizes: List[int] = [16, 32, 40],\n",
    "    overlaps: List[int] = [8, 16, 20],\n",
    "    max_pairs: int = None,\n",
    "    neg_per_anchor: int = 2,\n",
    "    neg_strategy: str = 'other_chapter',   # 'other_chapter' or 'global_random'\n",
    "    seed: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns (passages, train_pairs_text)\n",
    "    - passages: list of passages (with chapter + text)\n",
    "    - train_pairs_text: list of dicts {'anchor': text, 'positive': text, 'negatives': [texts], 'meta': ...}\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    chapters = normalize_chapters_from_dict(chapters_raw)\n",
    "    passages = []\n",
    "    chapter_to_passages = {}\n",
    "\n",
    "    # build windows per chapter and register passage texts\n",
    "    for ch_id, verses in chapters.items():\n",
    "        windows_by_size = {}\n",
    "        for W, O in zip(window_sizes, overlaps):\n",
    "            windows = build_windows_for_chapter(verses, W, O)\n",
    "            # tag chapter on windows\n",
    "            for w in windows:\n",
    "                w[\"chapter\"] = ch_id\n",
    "            windows_by_size[W] = windows\n",
    "\n",
    "        # flatten and collect passage texts\n",
    "        chapter_pass_texts = []\n",
    "        for W in window_sizes:\n",
    "            for w in windows_by_size[W]:\n",
    "                passages.append({\"chapter\": ch_id, \"window_size\": W, \"start_idx\": w[\"start_idx\"], \"end_idx\": w[\"end_idx\"], \"verse_count\": w[\"verse_count\"], \"text\": w[\"text\"]})\n",
    "                chapter_pass_texts.append(w[\"text\"])\n",
    "        chapter_to_passages[ch_id] = chapter_pass_texts\n",
    "\n",
    "    # create positive pairs (within-chapter)\n",
    "    train_pairs = []\n",
    "    for ch_id, verses in chapters.items():\n",
    "        # rebuild windows_by_size for this chapter only to create positive pairs (ensures adjacency inside chapter)\n",
    "        windows_by_size = {}\n",
    "        for W, O in zip(window_sizes, overlaps):\n",
    "            windows = build_windows_for_chapter(chapters[ch_id], W, O)\n",
    "            windows_by_size[W] = windows\n",
    "        pairs = create_positive_pairs_text(windows_by_size)\n",
    "        # attach chapter meta (optional)\n",
    "        for p in pairs:\n",
    "            p[\"meta\"][\"chapter\"] = ch_id\n",
    "        train_pairs.extend(pairs)\n",
    "\n",
    "    # shuffle & cap\n",
    "    random.shuffle(train_pairs)\n",
    "    if max_pairs and max_pairs < len(train_pairs):\n",
    "        train_pairs = train_pairs[:max_pairs]\n",
    "\n",
    "    # prepare negative sampling pool (texts)\n",
    "    all_texts = [p[\"text\"] for p in passages]\n",
    "    # chapter -> texts already in chapter_to_passages\n",
    "\n",
    "    def sample_neg_texts(anchor_ch, exclude_texts, n, strategy):\n",
    "        pool = []\n",
    "        if strategy == 'other_chapter':\n",
    "            other_ch = [c for c in chapter_to_passages.keys() if c != anchor_ch]\n",
    "            for c in other_ch:\n",
    "                pool.extend(chapter_to_passages[c])\n",
    "        else:\n",
    "            pool = all_texts\n",
    "        # exclude anchor/positive\n",
    "        candidates = [t for t in pool if t not in exclude_texts]\n",
    "        if not candidates:\n",
    "            return []\n",
    "        sampled = []\n",
    "        tries = 0\n",
    "        while len(sampled) < n and tries < n * 10:\n",
    "            choice = random.choice(candidates)\n",
    "            if choice not in sampled:\n",
    "                sampled.append(choice)\n",
    "            tries += 1\n",
    "        return sampled\n",
    "\n",
    "    # attach negatives (as full texts) to each pair\n",
    "    train_pairs_text = []\n",
    "    for tp in train_pairs:\n",
    "        anchor_text = tp[\"anchor\"]\n",
    "        positive_text = tp[\"positive\"]\n",
    "        anchor_ch = tp[\"meta\"].get(\"chapter\")  # chapter where pair came from\n",
    "        negs = sample_neg_texts(anchor_ch, [anchor_text, positive_text], neg_per_anchor, neg_strategy)\n",
    "        train_pairs_text.append({\"anchor\": anchor_text, \"positive\": positive_text, \"negatives\": negs, \"meta\": tp[\"meta\"]})\n",
    "\n",
    "    return passages, train_pairs_text\n",
    "\n",
    "# %% ===== USER: provide your chapters_dict here (replace the mock) =====\n",
    "# Replace this small mock with your real in-memory dict of 1700 chapters.\n",
    "chapters_dict = {\n",
    "    \"chap_001\": [\n",
    "        \"Vyasa said: In the assembly the king asked for recitation of the lineage.\",\n",
    "        \"Sanjaya described the kings of the Bharatas and their deeds.\",\n",
    "        \"The sages listened and noted the names of heroic warriors.\",\n",
    "        \"Yudhisthira was praised for his righteousness.\",\n",
    "        \"Arjuna swore to fight for dharma.\"\n",
    "    ],\n",
    "    \"chap_002\": [\n",
    "        \"Duryodhana plotted in secret, gathering his allies by the river.\",\n",
    "        \"Karna pledged his support and promised weapons and chariots.\",\n",
    "        \"A messenger brought news of an alliance to Hastinapura.\",\n",
    "        \"Draupadi learned of the plot and stood in quiet resolve.\"\n",
    "    ],\n",
    "    # ... replace with your full chapters dictionary ...\n",
    "}\n",
    "\n",
    "# %% Build the simple dataset (change params if you want)\n",
    "out_dir = \"./dataset_out_simple\"\n",
    "ensure_dir(out_dir)\n",
    "\n",
    "window_sizes = [16, 32, 40]   # recommended: 40 max (fits your ~33 avg)\n",
    "overlaps = [8, 16, 20]\n",
    "max_pairs = 50000\n",
    "neg_per_anchor = 2\n",
    "neg_strategy = 'other_chapter'\n",
    "seed = 42\n",
    "\n",
    "passages, train_pairs_text = build_simple_dataset(\n",
    "    chapters,\n",
    "    window_sizes=window_sizes,\n",
    "    overlaps=overlaps,\n",
    "    max_pairs=max_pairs,\n",
    "    neg_per_anchor=neg_per_anchor,\n",
    "    neg_strategy=neg_strategy,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "print(f\"Built {len(passages)} passages and {len(train_pairs_text)} pairs (each with {neg_per_anchor} negatives)\")\n",
    "\n",
    "# %% Save simple outputs (full texts)\n",
    "save_jsonl(passages, os.path.join(out_dir, \"passages.jsonl\"))\n",
    "save_jsonl(train_pairs_text, os.path.join(out_dir, \"train_pairs.jsonl\"))\n",
    "\n",
    "# preview (first 20)\n",
    "preview = train_pairs_text[:20]\n",
    "save_jsonl(preview, os.path.join(out_dir, \"preview_pairs.jsonl\"))\n",
    "\n",
    "print(\"Saved files to\", out_dir)\n",
    "\n",
    "# %% print compact preview\n",
    "for i, p in enumerate(preview):\n",
    "    print(f\"--- PAIR {i+1} ---\")\n",
    "    print(\"Anchor (excerpt):\", p['anchor'][:300].replace('\\n',' | '))\n",
    "    print(\"Positive (excerpt):\", p['positive'][:300].replace('\\n',' | '))\n",
    "    print(\"Negatives count:\", len(p['negatives']))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
